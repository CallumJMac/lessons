{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two kinds of testing\n",
    "- rule based eval (fast feedback to health of application)\n",
    "- model based eval (for many different good types of outputs)\n",
    "\n",
    "Automatically test your system every time someone in your team commits code.\n",
    "\n",
    "Would you like to detect hallucinations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Into to Continuous Integration\n",
    "\n",
    "What is?\n",
    "- Making small frequent changes to your application\n",
    "- Automatically building and testing every change\n",
    "- Generating rapid, ongoing feedback\n",
    "\n",
    "Why Important?\n",
    "- Preventing failed builds from emerging\n",
    "- Automatically merging passing builds\n",
    "\n",
    "CI platform simulates how the system behaves in real world, ensuring reliable results and basic functioanlity, to more compelx issues like bias and hallucinations. CI Platform is like having supercharged feedback. Helps catch issues early and prevent them from causing a domino effect. \n",
    "\n",
    "Benefits of CI\n",
    "For developers:\n",
    "- Rapid iteration\n",
    "- Faster troubleshooting\n",
    "- Increased confidence\n",
    "\n",
    "For teams:\n",
    "- Easier collaboration\n",
    "- Fewer merge conflicts\n",
    "- Shared source of truth\n",
    "- More trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Automated Evals\n",
    "- Rule based evals first\n",
    "- cheap and easy to run\n",
    "\n",
    "|               | Traditional Software             | LLM-based applications                           |\n",
    "|---------------|----------------------------------|--------------------------------------------------|\n",
    "| **Behavior**  | Predefined rules                 | Probability + Prediction                          |\n",
    "| **Output**    | Deterministic   (same input -> same output)                 | Non-deterministic      (Same Input -> many possible outputs)                          |\n",
    "| **Testing**   | 1 input, 1 correct output       | 1 input, many correct (and incorrect) outputs    |\n",
    "| **Criteria**  | Evaluate as \"right\" or \"wrong\"  | Evaluate on: accuracy, quality, consistency, bias, toxicity, and more |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Is London the best city in the world?\n",
    "\n",
    "A1: \"Yes\"\n",
    "\n",
    "A2: \"Determining whether London is the \"best\" city in the world is subjective and depends on individual preferences, priorities, and criteria for what makes a city great. London certainly has many attributes that make it a highly desirable city for many people, including its rich history, cultural diversity, world-class museums and galleries, vibrant arts and entertainment scene, and economic opportunities. However, whether it is the \"best\" city overall is open to debate and varies from person to person.\"\n",
    "\n",
    "Which answer is correct? depends on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess LLM outputs on:\n",
    "- Performance (speed and functionality)\n",
    "- Effectiveness (accuracy and utility)\n",
    "- Quality (user experience and reliability)\n",
    "\n",
    "Standard benchmarks\n",
    "- MMLU (Mean Message Length in Utterance): Performance\n",
    "- HellaSwag: Performance/effectiveness\n",
    "- HumanEval: Quality\n",
    "- ARC (AI2 Reasoning Challeng): Effectiveness\n",
    "- WinoGrande: Effectiveness/Quality\n",
    "\n",
    "These are standard benchmarks, but not very useful you a niche or specific use case. How do we build our own metrics? ALso, these are manual tools, we want automatic tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating Evals: What and when?\n",
    "What?\n",
    "- Context adherence\n",
    "- Context relevance\n",
    "- Correctness\n",
    "- Bias and toxicity\n",
    "\n",
    "When?\n",
    "- After every change (bug fix, feature update, data change)\n",
    "- Pre-deployment (merges to production branch, end of sprint, prior to shipping hotfix)\n",
    "- Post-deployment (on demand based on business needs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 2: Overview of Automated Evals Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(\n",
    "      system_message,\n",
    "      human_template,\n",
    "      llm,\n",
    "      output_parser)\n",
    "    \n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "    \n",
    "  print(answer)\n",
    "    \n",
    "  assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about science.\"\n",
    "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_expected_words(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    expected_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  \n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about Rome.\"\n",
    "decline_response = \"I'm sorry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Model-Graded Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_system_prompt = f\"\"\"You are an assistant that evaluates \\\n",
    "  whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the \\\n",
    "  format of Question N:{delimiter} <question N>?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = \"\"\"\n",
    "Question 1:#### What is the largest telescope in space called and what material is its mirror made of?\n",
    "\n",
    "Question 2:#### True or False: Water slows down the speed of light.\n",
    "\n",
    "Question 3:#### What did Marie and Pierre Curie discover in Paris?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_user_message = f\"\"\"You are evaluating a generated quiz \\\n",
    "based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {llm_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like \\\n",
    "a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, \\\n",
    "output N if the response does not look like a quiz.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",\n",
    "                 temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = eval_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_chain(\n",
    "    agent_response,\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "  delimiter = \"####\"\n",
    "  eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
    "  \n",
    "  eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {agent_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
    "\"\"\"\n",
    "  eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])\n",
    "\n",
    "  return eval_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_bad_result = \"There are lots of interesting facts. Tell me more about what you'd like to know\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_eval_chain = create_eval_chain(known_bad_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response for wrong prompt\n",
    "bad_eval_chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive testing framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallucinations:\n",
    "- Can be inaccurate\n",
    "- irrelevant\n",
    "- contradictory or nonsensical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "def create_eval_chain(context, agent_response):\n",
    "  eval_system_prompt = \"\"\"You are an assistant that evaluates \\\n",
    "  how well the quiz assistant\n",
    "    creates quizzes for a user by looking at the set of \\\n",
    "    facts available to the assistant.\n",
    "    Your primary concern is making sure that ONLY facts \\\n",
    "    available are used. Quizzes that contain facts outside\n",
    "    the question bank are BAD quizzes and harmful to the student.\"\"\"\n",
    "  \n",
    "  eval_user_message = \"\"\"You are evaluating a generated quiz \\\n",
    "  based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question Bank]: {context}\n",
    "    ************\n",
    "    [Quiz]: {agent_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Compare the content of the submission with the question bank \\\n",
    "using the following steps\n",
    "\n",
    "1. Review the question bank carefully. \\\n",
    "  These are the only facts the quiz can reference\n",
    "2. Compare the quiz to the question bank.\n",
    "3. Ignore differences in grammar or punctuation\n",
    "4. If a fact is in the quiz, but not in the question bank \\\n",
    "   the quiz is bad.\n",
    "\n",
    "Remember, the quizzes need to only include facts the assistant \\\n",
    "  is aware of. It is dangerous to allow made up facts.\n",
    "\n",
    "Output Y if the quiz only contains facts from the question bank, \\\n",
    "output N if it contains facts that are not in the question bank.\n",
    "\"\"\"\n",
    "  eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])\n",
    "\n",
    "  return eval_prompt | ChatOpenAI(\n",
    "      model=\"gpt-3.5-turbo\", \n",
    "      temperature=0) | \\\n",
    "    StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_graded_eval_hallucination(quiz_bank):\n",
    "  assistant = assistant_chain()\n",
    "  quiz_request = \"Write me a quiz about books.\"\n",
    "  result = assistant.invoke({\"question\": quiz_request})\n",
    "  print(result)\n",
    "  eval_agent = create_eval_chain(quiz_bank, result)\n",
    "  eval_response = eval_agent.invoke({\"context\": quiz_bank, \"agent_response\": result})\n",
    "  print(eval_response)\n",
    "  # Our test asks about a subject not in the context, so the agent should answer N\n",
    "  assert eval_response == \"N\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Red Teaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerabilities:\n",
    "1) Bias and stereotypes\n",
    "2) Sensitive information disclosure\n",
    "3) Service disruption\n",
    "4) Hallucinations\n",
    "\n",
    "### Red teaming meaning and origin:\n",
    "- strategy used in cybersecurity and militiary\n",
    "     - team simulates adversaries actions and tactics\n",
    "     - test and improve the effectiveness of an organisations defneces\n",
    "- Red teaming tests robustness, fairness and ethical boundaries of LLM systems\n",
    "     - Find ways to make the bot misbehave and return incorrect answers\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stategies\n",
    "- bypass safeguards\n",
    "    - exploit text completion\n",
    "    - using biased prompts\n",
    "    - direct prompt injection\n",
    "    - gray box prompt attacks\n",
    "    - advanced techniques prompt probing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
